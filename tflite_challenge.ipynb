{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajgquional/LiL_Learning-TinyML/blob/main/tflite_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimization Challenge\n",
        "\n",
        "In this challenge, you will have to quantize a trained model and check for changes in model metrics. You will have to quantize your model into both INT8 and FLOAT16 and then compare the drop in accuracy and reduction in model size.\n",
        "\n",
        "There are cells to train a model on the MNIST dataset. There is also a function that you can use to calculate the accuracy of the model.\n",
        "\n",
        "You will have to finish the cells with TODO. You can expand the cells in the solution notebook if you get stuck or to verify your answer."
      ],
      "metadata": {
        "id": "SZFtwHs3-iLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Packages\n",
        "\n",
        "The first step is to import the packages we need. We will be using `tensorflow` and `keras` to train our models. We will also be using `numpy` to evaluate the performance of our model. Later on, we will be using `os` to find the size of our original and quantized models."
      ],
      "metadata": {
        "id": "bRUe0IXAQ_v-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "tpSTE_88EAL9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a model on MNIST\n",
        "TensorFlow provides an easy API to download the MNIST dataset and separate it into training and testing dataset. So first we will use that to load our data.\n",
        "\n",
        "We will preprocess our data by dividing all the pixel values by 255 to normalize them.\n",
        "\n",
        "Next we will create a simple CNN model with two convolutional layers and two dense layers.\n",
        "\n",
        "Finally, we can compile and train our model.\n"
      ],
      "metadata": {
        "id": "-Mmrj5FUSriV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 to 1.\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Creating the model\n",
        "model = keras.Sequential([\n",
        "  keras.layers.InputLayer(input_shape=(28, 28)),\n",
        "  keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "  keras.layers.Conv2D(filters=24, kernel_size=(3, 3), activation='relu'),\n",
        "  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
        "  keras.layers.Flatten(),\n",
        "  keras.layers.Dense(32, activation='relu'),\n",
        "  keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "# Training the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  epochs=2,\n",
        "  validation_split=0.1,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mw7OpDzsEC1Q",
        "outputId": "21be8f87-a46f-4ea1-bf97-59d8936b6c7d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/2\n",
            "1688/1688 [==============================] - 62s 36ms/step - loss: 0.1856 - accuracy: 0.9431 - val_loss: 0.0646 - val_accuracy: 0.9820\n",
            "Epoch 2/2\n",
            "1688/1688 [==============================] - 61s 36ms/step - loss: 0.0587 - accuracy: 0.9827 - val_loss: 0.0590 - val_accuracy: 0.9855\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f20c76a2190>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Model Performance\n",
        "Below is a function that take a tflite interpreter and runs inference on the test set of the MNIST data. We can use this to evaluate the performance of our original and quantized models"
      ],
      "metadata": {
        "id": "6PFy6lKnVpLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(interpreter):\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  # Run predictions on every image in the \"test\" dataset.\n",
        "  prediction_digits = []\n",
        "  for i, test_image in enumerate(test_images):\n",
        "    if i % 1000 == 0:\n",
        "      print('Evaluated on {n} results so far.'.format(n=i))\n",
        "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "    # the model's input data format.\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "    interpreter.set_tensor(input_index, test_image)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the digit with highest\n",
        "    # probability.\n",
        "    output = interpreter.tensor(output_index)\n",
        "    digit = np.argmax(output()[0])\n",
        "    prediction_digits.append(digit)\n",
        "\n",
        "  print('\\n')\n",
        "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "  prediction_digits = np.array(prediction_digits)\n",
        "  accuracy = (prediction_digits == test_labels).mean()\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "qKpVR7SgLysv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: Quantize the model weights to INT8\n",
        "\n",
        "In the cell below, write code that take the keras `model` we trained before and then performs weight quanization to INT8.\n",
        "\n",
        "Then create a `tflite` interpreter and use the `evaluate_model()` function to check the accuracy of the model"
      ],
      "metadata": {
        "id": "I04qVe20V7MD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO Perform Weight Quantization"
      ],
      "metadata": {
        "id": "rREJj-n7W0QC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO Evaluate Model Performance"
      ],
      "metadata": {
        "id": "5BD_ZrHrW36c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution: Quantize the model weights to INT8\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "int8_quantized_tflite_model = converter.convert()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpvG4tWm894L",
        "outputId": "f22247d7-7f41-4721-aaf0-bfc74b9051bb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution: Evaluate the model performance\n",
        "interpreter = tf.lite.Interpreter(model_content=int8_quantized_tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "test_accuracy = evaluate_model(interpreter)\n",
        "\n",
        "print('INT8 Quantized Model Test Accuracy:', test_accuracy)\n",
        "\n",
        "# quantized model has dropped to 98.18% accuracy compared to the 98.68% accuracy of the orginal model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUpJLpVI9l8E",
        "outputId": "7cdb7aa8-5140-420b-8440-dc62e2b325df"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluated on 0 results so far.\n",
            "Evaluated on 1000 results so far.\n",
            "Evaluated on 2000 results so far.\n",
            "Evaluated on 3000 results so far.\n",
            "Evaluated on 4000 results so far.\n",
            "Evaluated on 5000 results so far.\n",
            "Evaluated on 6000 results so far.\n",
            "Evaluated on 7000 results so far.\n",
            "Evaluated on 8000 results so far.\n",
            "Evaluated on 9000 results so far.\n",
            "\n",
            "\n",
            "INT8 Quantized Model Test Accuracy: 0.9818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: Quantize the model weights to FLOAT16\n",
        "\n",
        "In the cell below, write code that take the keras `model` we trained before and then performs weight quanization to FLOAT16.\n",
        "\n",
        "Then create a `tflite` interpreter and use the `evaluate_model()` function to check the accuracy of the model"
      ],
      "metadata": {
        "id": "odsGwi55Xr_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO Perform Weight Quantization"
      ],
      "metadata": {
        "id": "NRh2YplvXr_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO Evaluate Model Performance"
      ],
      "metadata": {
        "id": "axXOd2pzXr_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution: Quantize the model weights to FLOAT16\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "float16_quantized_tflite_model = converter.convert()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3GWAtBd-Xuw",
        "outputId": "294ea2f9-d277-4aef-8a24-de8dbe9ae782"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution: Evaluate the model performance\n",
        "interpreter = tf.lite.Interpreter(model_content=float16_quantized_tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "test_accuracy = evaluate_model(interpreter)\n",
        "\n",
        "print('FLOAT16 Quantized Model Test Accuracy:', test_accuracy)\n",
        "\n",
        "# accuracy of the FLOAT16 model is 98.16%"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ua0OWZKb-_kR",
        "outputId": "21dc335d-3949-48a2-a89d-1a6948805574"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluated on 0 results so far.\n",
            "Evaluated on 1000 results so far.\n",
            "Evaluated on 2000 results so far.\n",
            "Evaluated on 3000 results so far.\n",
            "Evaluated on 4000 results so far.\n",
            "Evaluated on 5000 results so far.\n",
            "Evaluated on 6000 results so far.\n",
            "Evaluated on 7000 results so far.\n",
            "Evaluated on 8000 results so far.\n",
            "Evaluated on 9000 results so far.\n",
            "\n",
            "\n",
            "FLOAT16 Quantized Model Test Accuracy: 0.9816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO: Save the model files and calculate their size\n",
        "In the cell below, write code to save both the original and quantized model files and then calculate the model size. You can use the `os.path.getsize()` function to get the size of a file."
      ],
      "metadata": {
        "id": "EoIkQLYtYGRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO Calculate the model size"
      ],
      "metadata": {
        "id": "m1sD-qUizj4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution: Save model file and calculate their size\n",
        "\n",
        "original_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "original_tflite_model = original_converter.convert()\n",
        "\n",
        "with open('original_model', 'wb') as f:\n",
        "  f.write(original_tflite_model)\n",
        "\n",
        "with open('int8_model', 'wb') as f:\n",
        "  f.write(int8_quantized_tflite_model)\n",
        "\n",
        "with open('float16_model', 'wb') as f:\n",
        "  f.write(float16_quantized_tflite_model)\n",
        "\n",
        "print(\"Original model size in Mb:\", os.path.getsize('original_model') / float(2**20))\n",
        "print(\"INT8 model size in Mb:\", os.path.getsize('int8_model') / float(2**20))\n",
        "print(\"FLOAT16 model size in Mb:\", os.path.getsize('float16_model') / float(2**20))"
      ],
      "metadata": {
        "id": "NbVMqOh_7z0_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1787be81-ceaa-4932-a5ed-e4624ebe9d0e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original model size in Mb: 0.8595390319824219\n",
            "INT8 model size in Mb: 0.21955108642578125\n",
            "FLOAT16 model size in Mb: 0.4326019287109375\n"
          ]
        }
      ]
    }
  ]
}